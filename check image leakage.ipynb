{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c67c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import hashlib\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import imagehash \n",
    "from imagehash import phash \n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dd4c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7931247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(PROJECT_ROOT, 'train_folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20194bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_path = os.path.join(PROJECT_ROOT, 'external_folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f378613f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2008/2008 [00:06<00:00, 299.40it/s]\n",
      "100%|██████████| 5000/5000 [00:14<00:00, 346.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicated image files in external set: 1649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def perceptual_image_hash(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path).convert('L') as img:\n",
    "            img = img.resize((224, 224))\n",
    "            #return hashlib.md5(img.tobytes()).hexdigest()\n",
    "            return str(imagehash.phash(img))\n",
    "    except Exception as e:\n",
    "        print(f\"Error with image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_image_hashes(image_folder):\n",
    "    image_paths = glob(os.path.join(image_folder, \"**\", '*.*'), recursive = True)\n",
    "    #print(f\"Scanning {len(image_paths)} images in {image_folder}..\")\n",
    "    hashes = {}\n",
    "    for path in tqdm(image_paths):\n",
    "        h = perceptual_image_hash(path)\n",
    "        if h:\n",
    "            if h not in hashes:\n",
    "                hashes[h] = [path]\n",
    "            else:\n",
    "                hashes[h].append(path)\n",
    "    return hashes\n",
    "\n",
    "train_folder = train_path\n",
    "external_folder = external_path\n",
    "\n",
    "train_hashes = get_image_hashes(train_folder)\n",
    "external_hashes = get_image_hashes(external_folder)\n",
    "\n",
    "# detect overlapping hashes\n",
    "train_hash_set = set(train_hashes.keys())\n",
    "external_hash_set = set(external_hashes.keys())\n",
    "overlap_hashes = train_hash_set.intersection(external_hash_set)\n",
    "\n",
    "\n",
    "\n",
    "# gather all duplicated external image paths\n",
    "\n",
    "duplicate_external_path = []\n",
    "for h in overlap_hashes:\n",
    "    duplicate_external_path.extend(external_hashes[h])\n",
    "\n",
    "\n",
    "print(f\"Total duplicated image files in external set: {len(duplicate_external_path)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82eab04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_raw_path_tumor = os.path.join(PROJECT_ROOT, 'brain tumor external raw/Tumor')\n",
    "external_path_tumor = os.path.join(PROJECT_ROOT, 'brain tumor external data/Tumor')\n",
    "external_raw_path_no_tumor = os.path.join(PROJECT_ROOT, 'brain tumor external raw/Healthy')\n",
    "external_path_no_tumor = os.path.join(PROJECT_ROOT, 'brain tumor external data/Healthy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "916eb266",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(external_path_tumor, exist_ok=True)\n",
    "os.makedirs(external_path_no_tumor, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8ccd6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2008/2008 [00:06<00:00, 308.34it/s]\n",
      "100%|██████████| 3000/3000 [00:09<00:00, 307.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicated image files in external tumor set: 877\n",
      "Copying 2123 non-duplicated tumor images to cleaned folder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying cleaned tumor images: 100%|██████████| 2123/2123 [00:00<00:00, 14882.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# check external image (tumor) against training images (tumor + no-tumor)\n",
    "def perceptual_image_hash(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path).convert('L') as img:\n",
    "            img = img.resize((224, 224))\n",
    "            #return hashlib.md5(img.tobytes()).hexdigest()\n",
    "            return str(imagehash.phash(img))\n",
    "    except Exception as e:\n",
    "        print(f\"Error with image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_image_hashes(image_folder):\n",
    "    image_paths = glob(os.path.join(image_folder, \"**\", '*.*'), recursive = True)\n",
    "    #print(f\"Scanning {len(image_paths)} images in {image_folder}..\")\n",
    "    hashes = {}\n",
    "    for path in tqdm(image_paths):\n",
    "        h = perceptual_image_hash(path)\n",
    "        if h:\n",
    "            if h not in hashes:\n",
    "                hashes[h] = [path]\n",
    "            else:\n",
    "                hashes[h].append(path)\n",
    "    return hashes\n",
    "\n",
    "#train_folder = train_path\n",
    "#external_folder_tumor = external_path_tumor\n",
    "\n",
    "train_hashes = get_image_hashes(train_path)\n",
    "external_hashes_tumor = get_image_hashes(external_raw_path_tumor)\n",
    "\n",
    "# detect overlapping hashes\n",
    "train_hash_set = set(train_hashes.keys())\n",
    "external_hash_set_tumor = set(external_hashes_tumor.keys())\n",
    "overlap_hashes_tumor = train_hash_set.intersection(external_hash_set_tumor)\n",
    "\n",
    "# gather all duplicated external image paths\n",
    "\n",
    "duplicate_external_tumor_path = []\n",
    "for h in overlap_hashes_tumor:\n",
    "    duplicate_external_tumor_path.extend(external_hashes_tumor[h])\n",
    "\n",
    "\n",
    "print(f\"Total duplicated image files in external tumor set: {len(duplicate_external_tumor_path)}\")\n",
    "\n",
    "# copy no-duplicated images to cleaned folder\n",
    "all_external_paths = [p for paths in external_hashes_tumor.values() for p in paths]\n",
    "non_duplicated_paths = set(all_external_paths)- set(duplicate_external_tumor_path)\n",
    "print(f\"Copying {len(non_duplicated_paths)} non-duplicated tumor images to cleaned folder\")\n",
    "\n",
    "for path in tqdm(non_duplicated_paths, desc='Copying cleaned tumor images'):\n",
    "    filename = os.path.basename(path)\n",
    "    desc_path = os.path.join(external_path_tumor, filename)\n",
    "    try:\n",
    "        shutil.copy2(path, desc_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to copy {path} to {desc_path}: e\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1acb1be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2008/2008 [00:06<00:00, 308.92it/s]\n",
      "100%|██████████| 2000/2000 [00:04<00:00, 422.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicated image files in external no tumor set: 772\n",
      "Copying 1228 non-duplicated no tumor images to cleaned folder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying cleaned no tumor images: 100%|██████████| 1228/1228 [00:00<00:00, 16422.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# check external image (no-tumor) against training images (tumor + no-tumor)\n",
    "def perceptual_image_hash(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path).convert('L') as img:\n",
    "            img = img.resize((224, 224))\n",
    "            #return hashlib.md5(img.tobytes()).hexdigest()\n",
    "            return str(imagehash.phash(img))\n",
    "    except Exception as e:\n",
    "        print(f\"Error with image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_image_hashes(image_folder):\n",
    "    image_paths = glob(os.path.join(image_folder, \"**\", '*.*'), recursive = True)\n",
    "    #print(f\"Scanning {len(image_paths)} images in {image_folder}..\")\n",
    "    hashes = {}\n",
    "    for path in tqdm(image_paths):\n",
    "        h = perceptual_image_hash(path)\n",
    "        if h:\n",
    "            if h not in hashes:\n",
    "                hashes[h] = [path]\n",
    "            else:\n",
    "                hashes[h].append(path)\n",
    "    return hashes\n",
    "\n",
    "\n",
    "\n",
    "train_hashes = get_image_hashes(train_path)\n",
    "external_hashes_no_tumor = get_image_hashes(external_raw_path_no_tumor)\n",
    "\n",
    "# detect overlapping hashes\n",
    "train_hash_set = set(train_hashes.keys())\n",
    "external_hash_set_no_tumor = set(external_hashes_no_tumor.keys())\n",
    "overlap_hashes_no_tumor = train_hash_set.intersection(external_hash_set_no_tumor)\n",
    "\n",
    "# gather all duplicated external image paths\n",
    "\n",
    "duplicate_external_no_tumor_path = []\n",
    "for h in overlap_hashes_no_tumor:\n",
    "    duplicate_external_no_tumor_path.extend(external_hashes_no_tumor[h])\n",
    "\n",
    "\n",
    "print(f\"Total duplicated image files in external no tumor set: {len(duplicate_external_no_tumor_path)}\")\n",
    "\n",
    "# copy no-duplicated images to cleaned folder\n",
    "all_external_paths = [p for paths in external_hashes_no_tumor.values() for p in paths]\n",
    "non_duplicated_paths = set(all_external_paths)- set(duplicate_external_no_tumor_path)\n",
    "print(f\"Copying {len(non_duplicated_paths)} non-duplicated no tumor images to cleaned folder\")\n",
    "\n",
    "for path in tqdm(non_duplicated_paths, desc='Copying cleaned no tumor images'):\n",
    "    filename = os.path.basename(path)\n",
    "    desc_path = os.path.join(external_path_no_tumor, filename)\n",
    "    try:\n",
    "        shutil.copy2(path, desc_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to copy {path} to {desc_path}: e\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
